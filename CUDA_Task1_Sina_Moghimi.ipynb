{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUDA_Task1_Sina_Moghimi.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WapUvuOmWz8"
      },
      "source": [
        "*   Student: Sina Moghimi\n",
        "*   University: [Moscow Institute of Physics and Technology (National Research University)](https://eng.mipt.ru)\n",
        "*   Group Number: лю01-108л░\n",
        "*   Area: 03.04.01 Applied Mathematics and Physics\n",
        "*   Field: Neural Networks & Neural Computers\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnHOYen2nSPY"
      },
      "source": [
        "In order to make this notebook work well, you should follow the instructions step by step.\n",
        "\n",
        "0- Since we have some files to work with, we store them in the google drive. Therefore, we have to connect colab to the google drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9RWPvKGterV",
        "outputId": "ba49f833-1936-411e-da14-02abf2ed3b88"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtkIZTwEtfnj"
      },
      "source": [
        "1- We need to run some initial commands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkajIGnf9_FE"
      },
      "source": [
        "!sudo apt update\n",
        "!sudo apt-get update\n",
        "# !apt-get install pciutils git curl build-essential\n",
        "!sudo apt autoremove\n",
        "\n",
        "from IPython.display import clear_output \n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRSmQ2VQlkgL"
      },
      "source": [
        "2- Since we run CUDA 10.1 in this task and the default version of the CUDA in colab is 11.1 by the time of writing this text, we will need to disable it first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVGUF3yHmDET",
        "outputId": "256188cd-bbfc-4bda-c295-969f25f598ce"
      },
      "source": [
        "%cd /usr/local\n",
        "!sudo rm cuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WKgHi0ekm8Y"
      },
      "source": [
        "3- To activate the CUDA 10.1, If you happend not to have CUDA-10.1 folder in the directory \"/usr/local/\" you should uncomment the commands below and install CUDA 10.1 first. Otherwise, just skip this step (step 3) and move on to the next step (step 4)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVnFyAaV_2Nw"
      },
      "source": [
        "# !sudo apt-get --purge remove \"*cublas*\" \"cuda*\" \"nsight*\"\n",
        "# 'Install CUDA 10.1 for Tesla K80'\n",
        "# %cd /content/drive/MyDrive/Colab Notebooks/Installation File\n",
        "# !pwd\n",
        "# !wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n",
        "# !sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "# ## !wget https://developer.download.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda-repo-ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb\n",
        "# !sudo dpkg -i cuda-repo-ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb\n",
        "# !sudo apt-key add /var/cuda-repo-10-1-local-10.1.243-418.87.00/7fa2af80.pub\n",
        "# !sudo apt-get update\n",
        "# !sudo apt-get -y install cuda-10-1\n",
        "# !sudo apt autoremove"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf-w4dxYoPWz"
      },
      "source": [
        "4- Set CUDA 10.1 as the default version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trhQVz33lZpc",
        "outputId": "cd85c8c9-dcc3-46f9-c768-f42f70a77fec"
      },
      "source": [
        "!sudo ln -s cuda-10.1 cuda\n",
        "print('DONE!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVDFKnCOojHn"
      },
      "source": [
        "5- Check CUDA and gcc version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlbTQqnh-3s2",
        "outputId": "ef471526-582d-4717-9938-34b3995d5992"
      },
      "source": [
        "!nvcc --version\n",
        "!echo \" \"\n",
        "!gcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            " \n",
            "gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
            "Copyright (C) 2017 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-gxjNy3ozbh"
      },
      "source": [
        "6- Prepare NVCC to run within the notebook cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shk-BZfSQBZY",
        "outputId": "820eee02-f94a-432c-c17b-eef4ad313b15"
      },
      "source": [
        "%cd /content/\n",
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%reload_ext nvcc_plugin\n",
        "\n",
        "clear_output()\n",
        "print('DONE!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzZOL4ZBgL_u"
      },
      "source": [
        "7- In order to run our code, we need to move the required files to `/content/include` directory. \n",
        "\n",
        "NOTE: All the required files are stored in the directory:\\\n",
        "/content/drive/MyDrive/Tasks/CUDA/task1/task1/Files/\n",
        "\n",
        "NOTE: You might need to modify the directory above with the directory where you stored your files on your drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-5b_Fq04pL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f92de72c-1ff3-4009-e0ad-982134da9ba1"
      },
      "source": [
        "%cd /content/drive/MyDrive/HWs/CUDA/task1/task1/Files\n",
        "! mkdir /content/include\n",
        "! cp input0.raw input1.raw output.raw wb.h /content/include\n",
        "print('DONE!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1IjpD87Nmoz-Jorw5N5Q6zc-eyuUC-HcY/task1/Files\n",
            "DONE!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8- It would be beneficial to hadle errors in CUDA, therefore we write a header file to do this for us."
      ],
      "metadata": {
        "id": "6ewJQ478ox1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda --name handle_error.cu\n",
        "#include \"cuda.h\"\n",
        "\n",
        "#define checkGPUError(res) {gpuAssert(res, __FILE__, __LINE__);}\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true){\n",
        "    if (code != cudaSuccess){\n",
        "        fprintf(stderr, \"GPU assert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "        if(abort) exit(code);\n",
        "    }\n",
        "}  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "kfuzsDElo7Bk",
        "outputId": "a771068f-c642-4461-c061-d6e2d61b4a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/handle_error.cu'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsDWDMY-rDaI"
      },
      "source": [
        "9- It's time to write the code. (To save all changes we make, we need to run the cell below at last.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "DB5X2bfCREqH",
        "outputId": "7136cdb4-d549-40b0-b009-7d216b10133b"
      },
      "source": [
        "%%cuda --name main.cu\n",
        "\n",
        "#include \"/content/include/wb.h\"\n",
        "#include \"handle_error.cu\"\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void vecAdd(float *in1, float *in2, float *out) {\n",
        "  //@@ Paste in the vector addition code\n",
        "  // DONE!\n",
        "  size_t L1_index{threadIdx.x + blockDim.x*(threadIdx.y + blockDim.y*threadIdx.z)};\n",
        "  size_t L2_index{blockIdx.x + gridDim.x*(blockIdx.y + gridDim.y*blockIdx.z)};\n",
        "  size_t block_size{blockDim.x*blockDim.y*blockDim.z};\n",
        "  size_t index{L1_index + block_size*L2_index};\n",
        "\n",
        "  out[index] = in1[index] + in2[index];\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "\n",
        "  int inputLength{};\n",
        "  float *hostInput1{};\n",
        "  float *hostInput2{};\n",
        "  float *hostOutput{};\n",
        "  float *deviceInput1{};\n",
        "  float *deviceInput2{};\n",
        "  float *deviceOutput{};\n",
        "\n",
        "  argc = 4;\n",
        "  argv = new char*[argc];\n",
        "  argv[0] = NULL;\n",
        "  argv[1] = \"/content/include/input0.raw\";\n",
        "  argv[2] = \"/content/include/input1.raw\";\n",
        "  argv[3] = \"/content/include/output.raw\";\n",
        "  wbArg_t args{wbArg_read(argc, argv)};\n",
        "\n",
        "  wbTime_start(Generic, \"Importing data and creating memory on host\");\n",
        "  hostInput1 = (float *)wbImport(wbArg_getInputFile(args, 0), &inputLength);\n",
        "  hostInput2 = (float *)wbImport(wbArg_getInputFile(args, 1), &inputLength);\n",
        "  hostOutput = new float[inputLength];\n",
        "  wbTime_stop(Generic, \"Importing data and creating memory on host\");\n",
        "\n",
        "  wbLog(TRACE, \"The input length is \", inputLength);\n",
        "\n",
        "  wbTime_start(GPU, \"Allocating GPU memory.\");\n",
        "  //@@ Allocating GPU memory code\n",
        "  // DONE!\n",
        "  size_t size_in_byte{inputLength * sizeof(float)};\n",
        "  checkGPUError(cudaMalloc((void **)&deviceInput1, size_in_byte));\n",
        "  checkGPUError(cudaMalloc((void **)&deviceInput2, size_in_byte));\n",
        "  checkGPUError(cudaMalloc((void **)&deviceOutput, size_in_byte));\n",
        "\n",
        "  wbTime_stop(GPU, \"Allocating GPU memory.\");\n",
        "\n",
        "  wbTime_start(GPU, \"Copying input memory to the GPU.\");\n",
        "  //@@ Paste your code\n",
        "  // DONE!\n",
        "  checkGPUError(cudaMemcpy(deviceInput1, hostInput1, size_in_byte, cudaMemcpyHostToDevice));\n",
        "  checkGPUError(cudaMemcpy(deviceInput2, hostInput2, size_in_byte, cudaMemcpyHostToDevice));\n",
        "  checkGPUError(cudaMemcpy(deviceOutput, hostOutput, size_in_byte, cudaMemcpyHostToDevice));\n",
        "\n",
        "  wbTime_stop(GPU, \"Copying input memory to the GPU.\");\n",
        "  //@@ Initialize grid and block dimensions\n",
        "  // DONE!\n",
        "  dim3 block_config(inputLength, 1, 1);\n",
        "  dim3 grid_config(1, 1, 1);\n",
        "  \n",
        "  wbTime_start(Compute, \"Performing CUDA computation\");\n",
        "  //@@ run GPU kernel\n",
        "  // DONE!\n",
        "  vecAdd<<<grid_config, block_config>>>(deviceInput1, deviceInput2, deviceOutput);\n",
        "\n",
        "  checkGPUError(cudaDeviceSynchronize());\n",
        "  \n",
        "  wbTime_stop(Compute, \"Performing CUDA computation\");\n",
        "\n",
        "  wbTime_start(Copy, \"Copying output memory to the CPU\");\n",
        "  //@@ copy memory from GPU back to host\n",
        "  // DONE!\n",
        "  checkGPUError(cudaMemcpy(hostOutput, deviceOutput, size_in_byte, cudaMemcpyDeviceToHost));\n",
        "  wbTime_stop(Copy, \"Copying output memory to the CPU\");\n",
        "\n",
        "  wbTime_start(GPU, \"Freeing GPU Memory\");\n",
        "  //@@ code to free memory on GPU\n",
        "  // DONE!\n",
        "  checkGPUError(cudaFree(deviceInput1));\n",
        "  checkGPUError(cudaFree(deviceInput2));\n",
        "  checkGPUError(cudaFree(deviceOutput));\n",
        "\n",
        "  wbTime_stop(GPU, \"Freeing GPU Memory\");\n",
        "\n",
        "  wbSolution(args, hostOutput, inputLength);\n",
        "\n",
        "  free(hostInput1);\n",
        "  free(hostInput2);\n",
        "  delete(hostOutput);\n",
        "  delete(argv);\n",
        "\n",
        "  cudaDeviceReset();\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/main.cu'"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP8tj0DRrVQw"
      },
      "source": [
        "10- The code is saved in the directory \"/content/src/\", So we make sure we are in the same directory and then we run the Code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwFEBu55kTAn",
        "outputId": "fe69f6a5-7bd0-4f1f-e35a-d0e8cccf3215"
      },
      "source": [
        "%cd /content/src/\n",
        "!nvcc -G main.cu -o run.out && ./run.out\n",
        "# !sudo rm /content/src/main.cu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/src\n",
            "main.cu(31): warning: conversion from a string literal to \"char *\" is deprecated\n",
            "\n",
            "main.cu(32): warning: conversion from a string literal to \"char *\" is deprecated\n",
            "\n",
            "main.cu(33): warning: conversion from a string literal to \"char *\" is deprecated\n",
            "\n",
            "main.cu(31): warning: conversion from a string literal to \"char *\" is deprecated\n",
            "\n",
            "main.cu(32): warning: conversion from a string literal to \"char *\" is deprecated\n",
            "\n",
            "main.cu(33): warning: conversion from a string literal to \"char *\" is deprecated\n",
            "\n",
            "[Generic] 0.000300032 Importing data and creating memory on host\n",
            "Trace main::42 The input length is 130\n",
            "[GPU    ] 0.000185088 Allocating GPU memory.\n",
            "[GPU    ] 0.000048128 Copying input memory to the GPU.\n",
            "[Compute] 0.000163840 Performing CUDA computation\n",
            "[Copy   ] 0.000043008 Copying output memory to the CPU\n",
            "[GPU    ] 0.000135168 Freeing GPU Memory\n",
            "Solution is correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11- By the command below wew can check the efficiency of our thread and grid configurations."
      ],
      "metadata": {
        "id": "TGBheNQqiC7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/src/\n",
        "!nvprof ./run.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saaETLFciAya",
        "outputId": "1798e140-0fad-41b5-b43f-12bc7f45846f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/src\n",
            "==2591== NVPROF is profiling process 2591, command: ./run.out\n",
            "[Generic] 0.000315136 Importing data and creating memory on host\n",
            "Trace main::42 The input length is 130\n",
            "[GPU    ] 0.000212992 Allocating GPU memory.\n",
            "[GPU    ] 0.000056832 Copying input memory to the GPU.\n",
            "[Compute] 0.000175104 Performing CUDA computation\n",
            "[Copy   ] 0.000047872 Copying output memory to the CPU\n",
            "[GPU    ] 0.000134912 Freeing GPU Memory\n",
            "Solution is correct.\n",
            "==2591== Profiling application: ./run.out\n",
            "==2591== Profiling result:\n",
            "No events/metrics were profiled.\n"
          ]
        }
      ]
    }
  ]
}